{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjzZX-T8kbgd"
      },
      "source": [
        "!pip install beautifulsoup\n",
        "!pip install urllib\n",
        "!pip install pandas"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUPlyFHEBfw0"
      },
      "source": [
        "# 필요한 라이브러리 import\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6OEyw-CEKVi"
      },
      "source": [
        "# 크롤링 결과를 저장할 리스트\n",
        "date_list = []\n",
        "title_list = []\n",
        "press_list = []\n",
        "contents_list = []\n",
        "link_list = []\n",
        "data_dict = {}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4mivICwufUd"
      },
      "source": [
        "# 기사의 내용을 정제화하는 함수 \n",
        "def contents_cleansing(contents):\n",
        "    # 앞에 있는 내용 중 필요없는 부분을 제거한다\n",
        "    first_step = re.sub('<dl>.*?</a> </div> </dd> <dd>', '', str(contents)).strip()\n",
        "    # 뒤에 있는 내용 중 필요없는 부분을 제거한다\n",
        "    second_step = re.sub('<ul class=\"relation_lst\">.*?</dd>', '', first_step).strip()\n",
        "    third_step = re.sub('<.+?>', '', second_step).strip()\n",
        "    # 정제화를 마친 내용을 기사 내용 담는 list에 append해준다.\n",
        "    contents_list.append(third_step)\n",
        "    \n",
        "\n",
        "def crawling(maxpage, keyword, sort, start, end):\n",
        "    page = 1\n",
        "    max =(int(maxpage) - 1) * 10 + 1\n",
        "    d_from = start.replace(\".\",\"\")\n",
        "    d_to = end.replace(\".\",\"\")\n",
        "    \n",
        "    while page <= max:\n",
        "        url = \"https://search.naver.com/search.naver?where=news&query=\" + keyword + \"&sort=\" + sort + \"&ds=\" + start + \"&de=\" + end + \"&nso=so%3Ar%2Cp%3Afrom\" + d_from + \"to\" + d_to + \"%2Ca%3A&start=\" + str(page)\n",
        "        response = requests.get(url)\n",
        "        html = response.text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        " \n",
        "        # <a> 태그에서 제목과 링크를 추출한다.\n",
        "        a_tag = soup.select('.news_tit')\n",
        "        for i in a_tag:\n",
        "            # 기사 제목 추출 \n",
        "            title_list.append(i.text)\n",
        "            # 기사 링크 추출\n",
        "            link_list.append(i['href'])\n",
        "            \n",
        "        # 신문사를 추출한다.\n",
        "        press_lists = soup.select('.info_group > .press')\n",
        "        for i in press_lists:\n",
        "            press_list.append(i.text)\n",
        "        \n",
        "        # 기사 날짜를 추출한다.\n",
        "        date_lists = soup.select('.info_group > span.info')\n",
        "        for i in date_lists:\n",
        "            # n면 n단에서 '면'을 제거하여 숫자만을 추출한다.\n",
        "            if i.text.find(\"면\") == -1:\n",
        "                date_list.append(i.text)\n",
        "        \n",
        "        # 기사 요약본을 추출한다.\n",
        "        contents_lists = soup.select('.news_dsc')\n",
        "        for i in contents_lists:\n",
        "            # 기사 요약본을 정제화할 함수를 호출한다.\n",
        "            contents_cleansing(i)\n",
        "        \n",
        "        # 리스트 형태로 저장된 data, title, press, contents, link를 data_dict에 딕셔너리 형태로 넣어준다.\n",
        "        data_dict= {\"date\" : date_list , \"title\":title_list ,  \"press\" : press_list ,\"contents\": contents_list ,\"link\":link_list }  \n",
        "        print(page)\n",
        "        \n",
        "        # 딕셔너리로 저장된 data_dict를  데이터프레임으로 변환해준다.\n",
        "        df = pd.DataFrame(data_dict)\n",
        "        page += 10\n",
        "        df\n",
        "    \n",
        "    \n",
        "    # to_csv를 사용하여 dataframe을 csv으로 생성\n",
        "    outputFileName = 'doosanbears_11'\n",
        "    df.to_csv(\"doosanbears_11.csv\")\n",
        "    \n",
        "\n",
        "def main():\n",
        "    maxpage = input(\"최대 페이지 수: \")  \n",
        "    keyword = input(\"키워드: \")  \n",
        "    sort = input(\"관련도순=0  최신순=1  오래된순=2: \")\n",
        "    start = input(\"시작 날짜 ex.(2021.04.03):\")\n",
        "    end = input(\"마지막 날짜 ex.(2021.11.18):\")\n",
        "    \n",
        "    crawling(maxpage, keyword, sort, start, end) \n",
        "    \n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}